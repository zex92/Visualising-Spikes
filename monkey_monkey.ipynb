{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as io\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.loadmat('monkeydata_training.mat', struct_as_record=False, squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_spike_data(spikes, bin_size_ms=20): 
    "    bin_size_points = int(bin_size_ms * (1000 / 1000))  # Assuming data sampled at 1000 Hz\n",
    "    n_neurons, n_timepoints = spikes.shape\n",
    "    n_bins = n_timepoints // bin_size_points\n",
    "    binned_spikes = np.zeros((n_neurons, n_bins))\n",
    "    for i in range(n_bins):\n",
    "        start = i * bin_size_points\n",
    "        end = start + bin_size_points\n",
    "        binned_spikes[:, i] = np.sum(spikes[:, start:end], axis=1)\n",
    "    return binned_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes_all_trials = []\n",
    "all_hand_positions = []\n",
    "\n",
    "for trial in data['trial']:\n",
    "    for angle in trial:\n",
    "        binned_spikes = bin_spike_data(angle.spikes)\n",
    "        binned_spikes_all_trials.append(binned_spikes)\n",
    "        all_hand_positions.append(angle.handPos[:2, :]) \n",
    "\n",
    "# Flatten hand position data to align with binned spikes\n",
    "all_hand_positions_flattened = [pos.reshape(-1) for pos in all_hand_positions]\n",
    "\n",
    "# Feature scaling and PCA\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21556\\4019543563.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(all_hand_positions_flattened)\n"
     ]
    }
   ],
   "source": [
    "X_pca_list = []\n",
    "\n",
    "for bs in binned_spikes_all_trials:\n",
    "    bs_scaled = scaler.fit_transform(bs.T)  # Transpose to get shape [timepoints, neurons] for scaling\n",
    "    bs_pca = pca.fit_transform(bs_scaled)  # Applying PCA\n",
    "    X_pca_list.append(bs_pca)\n",
    "\n",
    "# Padding for LSTM input\n",
    "max_len = max(x.shape[0] for x in X_pca_list)\n",
    "X_padded = np.array([np.pad(x, ((0, max_len - x.shape[0]), (0, 0)), 'constant', constant_values=0) for x in X_pca_list])\n",
    "\n",
    "# Preparing Target Variables\n",
    "y = np.array(all_hand_positions_flattened)\n",
    "max_len_y = max(y.shape[0] for y in all_hand_positions_flattened)\n",
    "y_padded = np.array([np.pad(y, (0, max_len_y - y.shape[0]), 'constant', constant_values=0) for y in all_hand_positions_flattened])\n",
    "\n",
    "# Scale target variables\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y_padded)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_scaled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(32, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    Dropout(0.4),\n",
    "    LSTM(32, activation='relu', return_sequences=False, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "    Dropout(0.4),\n",
    "    Dense(y_train.shape[1])\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 36ms/step - loss: 1.1387 - mae: 0.5977 - val_loss: 0.8115 - val_mae: 0.5881 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1364 - mae: 0.5987 - val_loss: 0.8082 - val_mae: 0.5887 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1329 - mae: 0.5985 - val_loss: 0.8063 - val_mae: 0.5901 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1314 - mae: 0.6009 - val_loss: 0.8071 - val_mae: 0.5895 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1301 - mae: 0.5987 - val_loss: 0.8065 - val_mae: 0.5894 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1284 - mae: 0.6002 - val_loss: 0.8048 - val_mae: 0.5926 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1245 - mae: 0.6060 - val_loss: 0.8034 - val_mae: 0.5926 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1221 - mae: 0.6009 - val_loss: 0.8023 - val_mae: 0.5932 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1236 - mae: 0.6054 - val_loss: 0.8017 - val_mae: 0.5912 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1174 - mae: 0.6035 - val_loss: 0.7974 - val_mae: 0.5994 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.1100 - mae: 0.6038 - val_loss: 0.7955 - val_mae: 0.5956 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0962 - mae: 0.6077 - val_loss: 0.7905 - val_mae: 0.5971 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.0814 - mae: 0.6076 - val_loss: 0.7901 - val_mae: 0.5889 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0798 - mae: 0.5961 - val_loss: 0.7888 - val_mae: 0.5928 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0448 - mae: 0.6066 - val_loss: 0.7887 - val_mae: 0.5915 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1757 - mae: 0.5992 - val_loss: 0.8052 - val_mae: 0.5788 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0949 - mae: 0.5903 - val_loss: 0.8012 - val_mae: 0.5918 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0426 - mae: 0.6056 - val_loss: 0.7945 - val_mae: 0.5965 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0408 - mae: 0.6086 - val_loss: 0.7914 - val_mae: 0.5829 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0359 - mae: 0.6026 - val_loss: 0.7936 - val_mae: 0.5903 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9423 - mae: 0.6059 - val_loss: 0.7913 - val_mae: 0.5875 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.9239 - mae: 0.6012 - val_loss: 0.7962 - val_mae: 0.5845 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0431 - mae: 0.5974 - val_loss: 0.7920 - val_mae: 0.5821 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2008 - mae: 0.6028 - val_loss: 0.7915 - val_mae: 0.5829 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 1.0463 - mae: 0.6058 - val_loss: 0.7850 - val_mae: 0.5935 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.9480 - mae: 0.5928 - val_loss: 0.7833 - val_mae: 0.5900 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8920 - mae: 0.5980 - val_loss: 0.7866 - val_mae: 0.5749 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9383 - mae: 0.5792 - val_loss: 0.7895 - val_mae: 0.5727 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8963 - mae: 0.5876 - val_loss: 0.7872 - val_mae: 0.5722 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9132 - mae: 0.5801 - val_loss: 0.7864 - val_mae: 0.5695 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8586 - mae: 0.5783 - val_loss: 0.8051 - val_mae: 0.5922 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8651 - mae: 0.5814 - val_loss: 0.7905 - val_mae: 0.5805 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9543 - mae: 0.5881 - val_loss: 0.7882 - val_mae: 0.5748 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8948 - mae: 0.5801 - val_loss: 0.7887 - val_mae: 0.5784 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8186 - mae: 0.5764 - val_loss: 0.7829 - val_mae: 0.5719 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8062 - mae: 0.5796 - val_loss: 0.7809 - val_mae: 0.5744 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9645 - mae: 0.5819 - val_loss: 0.7881 - val_mae: 0.5668 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.0570 - mae: 0.5706 - val_loss: 0.7845 - val_mae: 0.5696 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9981 - mae: 0.5762 - val_loss: 0.7868 - val_mae: 0.5670 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9588 - mae: 0.5756 - val_loss: 0.7823 - val_mae: 0.5689 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9673 - mae: 0.5733 - val_loss: 0.7815 - val_mae: 0.5738 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9238 - mae: 0.5749 - val_loss: 0.7740 - val_mae: 0.5707 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8570 - mae: 0.5759 - val_loss: 0.7794 - val_mae: 0.5652 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7758 - mae: 0.5669 - val_loss: 0.7737 - val_mae: 0.5687 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9439 - mae: 0.5706 - val_loss: 0.7708 - val_mae: 0.5689 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7684 - mae: 0.5694 - val_loss: 0.7721 - val_mae: 0.5614 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8587 - mae: 0.5676 - val_loss: 0.7690 - val_mae: 0.5644 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8129 - mae: 0.5662 - val_loss: 0.7826 - val_mae: 0.5763 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8010 - mae: 0.5766 - val_loss: 0.7782 - val_mae: 0.5713 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8320 - mae: 0.5700 - val_loss: 0.7716 - val_mae: 0.5686 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8990 - mae: 0.5729 - val_loss: 0.7803 - val_mae: 0.5607 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8118 - mae: 0.5672 - val_loss: 0.7723 - val_mae: 0.5650 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7929 - mae: 0.5660 - val_loss: 0.7710 - val_mae: 0.5658 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7645 - mae: 0.5661 - val_loss: 0.7708 - val_mae: 0.5648 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 1.0073 - mae: 0.5737 - val_loss: 0.7758 - val_mae: 0.5592 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.9298 - mae: 0.5651 - val_loss: 0.7736 - val_mae: 0.5683 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8748 - mae: 0.5680 - val_loss: 0.7733 - val_mae: 0.5621 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.9033 - mae: 0.5639 - val_loss: 0.7650 - val_mae: 0.5621 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.8018 - mae: 0.5664 - val_loss: 0.7650 - val_mae: 0.5607 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8251 - mae: 0.5632 - val_loss: 0.7640 - val_mae: 0.5593 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7741 - mae: 0.5627 - val_loss: 0.7658 - val_mae: 0.5611 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8577 - mae: 0.5678 - val_loss: 0.7713 - val_mae: 0.5563 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8169 - mae: 0.5605 - val_loss: 0.7623 - val_mae: 0.5587 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7739 - mae: 0.5616 - val_loss: 0.7661 - val_mae: 0.5654 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7481 - mae: 0.5612 - val_loss: 0.7666 - val_mae: 0.5596 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8792 - mae: 0.5626 - val_loss: 0.7618 - val_mae: 0.5583 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7587 - mae: 0.5587 - val_loss: 0.7660 - val_mae: 0.5576 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7487 - mae: 0.5631 - val_loss: 0.7630 - val_mae: 0.5535 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7702 - mae: 0.5556 - val_loss: 0.7624 - val_mae: 0.5613 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7694 - mae: 0.5561 - val_loss: 0.7617 - val_mae: 0.5539 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.8841 - mae: 0.5629 - val_loss: 0.7654 - val_mae: 0.5529 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7581 - mae: 0.5548 - val_loss: 0.7587 - val_mae: 0.5603 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7401 - mae: 0.5554 - val_loss: 0.7594 - val_mae: 0.5581 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7433 - mae: 0.5594 - val_loss: 0.7689 - val_mae: 0.5575 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7274 - mae: 0.5584 - val_loss: 0.7613 - val_mae: 0.5609 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7300 - mae: 0.5552 - val_loss: 0.7629 - val_mae: 0.5617 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7205 - mae: 0.5484 - val_loss: 0.7595 - val_mae: 0.5555 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7345 - mae: 0.5569 - val_loss: 0.7582 - val_mae: 0.5599 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7140 - mae: 0.5500 - val_loss: 0.7643 - val_mae: 0.5541 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8322 - mae: 0.5542 - val_loss: 0.7577 - val_mae: 0.5539 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7433 - mae: 0.5538 - val_loss: 0.7704 - val_mae: 0.5577 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7186 - mae: 0.5519 - val_loss: 0.7650 - val_mae: 0.5564 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8117 - mae: 0.5790 - val_loss: 0.7927 - val_mae: 0.5715 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9294 - mae: 0.5697 - val_loss: 0.7849 - val_mae: 0.5615 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7661 - mae: 0.5625 - val_loss: 0.7845 - val_mae: 0.5689 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7578 - mae: 0.5576 - val_loss: 0.7827 - val_mae: 0.5642 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7515 - mae: 0.5545 - val_loss: 0.7759 - val_mae: 0.5666 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7484 - mae: 0.5550 - val_loss: 0.7676 - val_mae: 0.5598 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7420 - mae: 0.5536 - val_loss: 0.7657 - val_mae: 0.5580 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8333 - mae: 0.5571 - val_loss: 0.7647 - val_mae: 0.5606 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7636 - mae: 0.5568 - val_loss: 0.7635 - val_mae: 0.5549 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7413 - mae: 0.5515 - val_loss: 0.7642 - val_mae: 0.5547 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.8164 - mae: 0.5563 - val_loss: 0.7625 - val_mae: 0.5559 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8669 - mae: 0.5606 - val_loss: 0.7595 - val_mae: 0.5587 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7387 - mae: 0.5520 - val_loss: 0.7580 - val_mae: 0.5528 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7524 - mae: 0.5513 - val_loss: 0.7610 - val_mae: 0.5545 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7495 - mae: 0.5516 - val_loss: 0.7543 - val_mae: 0.5506 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8323 - mae: 0.5554 - val_loss: 0.7523 - val_mae: 0.5558 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7231 - mae: 0.5467 - val_loss: 0.7510 - val_mae: 0.5526 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7988 - mae: 0.5512 - val_loss: 0.7518 - val_mae: 0.5521 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7510 - mae: 0.5497 - val_loss: 0.7608 - val_mae: 0.5546 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7919 - mae: 0.5527 - val_loss: 0.7566 - val_mae: 0.5508 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7833 - mae: 0.5464 - val_loss: 0.7530 - val_mae: 0.5490 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7046 - mae: 0.5436 - val_loss: 0.7541 - val_mae: 0.5524 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7073 - mae: 0.5406 - val_loss: 0.7490 - val_mae: 0.5504 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7830 - mae: 0.5466 - val_loss: 0.7523 - val_mae: 0.5532 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 21ms/step - loss: 0.7206 - mae: 0.5431 - val_loss: 0.7588 - val_mae: 0.5546 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8694 - mae: 0.5518 - val_loss: 0.7693 - val_mae: 0.5531 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.7183 - mae: 0.5402 - val_loss: 0.7574 - val_mae: 0.5493 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7653 - mae: 0.5457 - val_loss: 0.7523 - val_mae: 0.5475 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7073 - mae: 0.5395 - val_loss: 0.7603 - val_mae: 0.5513 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.7231 - mae: 0.5410 - val_loss: 0.7579 - val_mae: 0.5509 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7522 - mae: 0.5415 - val_loss: 0.7576 - val_mae: 0.5484 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9704 - mae: 0.5448 - val_loss: 0.7542 - val_mae: 0.5495 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7470 - mae: 0.5387 - val_loss: 0.7536 - val_mae: 0.5508 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6865 - mae: 0.5364 - val_loss: 0.7515 - val_mae: 0.5489 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.8271 - mae: 0.5425 - val_loss: 0.7523 - val_mae: 0.5496 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.9043 - mae: 0.5460 - val_loss: 0.7714 - val_mae: 0.5591 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8244 - mae: 0.5456 - val_loss: 0.7626 - val_mae: 0.5514 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7341 - mae: 0.5368 - val_loss: 0.7692 - val_mae: 0.5534 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7937 - mae: 0.5460 - val_loss: 0.7604 - val_mae: 0.5594 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6924 - mae: 0.5368 - val_loss: 0.7525 - val_mae: 0.5506 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7067 - mae: 0.5406 - val_loss: 0.7672 - val_mae: 0.5558 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7038 - mae: 0.5370 - val_loss: 0.7498 - val_mae: 0.5490 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7095 - mae: 0.5345 - val_loss: 0.7802 - val_mae: 0.5622 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7028 - mae: 0.5360 - val_loss: 0.7557 - val_mae: 0.5466 - lr: 5.0000e-04\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6846 - mae: 0.5330 - val_loss: 0.7588 - val_mae: 0.5504 - lr: 5.0000e-04\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6789 - mae: 0.5302 - val_loss: 0.7598 - val_mae: 0.5535 - lr: 5.0000e-04\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7281 - mae: 0.5342 - val_loss: 0.7532 - val_mae: 0.5481 - lr: 5.0000e-04\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.6805 - mae: 0.5327 - val_loss: 0.7576 - val_mae: 0.5504 - lr: 5.0000e-04\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.7975 - mae: 0.5376 - val_loss: 0.7617 - val_mae: 0.5538 - lr: 5.0000e-04\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6674 - mae: 0.5298 - val_loss: 0.7587 - val_mae: 0.5517 - lr: 5.0000e-04\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.6664 - mae: 0.5270 - val_loss: 0.7649 - val_mae: 0.5543 - lr: 5.0000e-04\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6656 - mae: 0.5264 - val_loss: 0.7597 - val_mae: 0.5513 - lr: 5.0000e-04\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6734 - mae: 0.5253 - val_loss: 0.7678 - val_mae: 0.5516 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7047961354255676, Test MAE: 0.5472999811172485\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test, y_test,verbose = 0)\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
